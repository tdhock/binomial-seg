\documentclass[12pt]{article}
\usepackage{tikz}
\usepackage[cm]{fullpage}
\usepackage{verbatim}
\usepackage{hyperref} 
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath,amssymb}
\usepackage{dsfont}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\HotSpots}{HotSpots}
\newcommand{\Cost}{\text{Cost}}
\DeclareMathOperator*{\Diag}{Diag}
\DeclareMathOperator*{\TPR}{TPR}
\DeclareMathOperator*{\FPR}{FPR}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}
\newcommand{\RR}{\mathbb R}

\begin{document}

\title{Constrained maximum likelihood binomial segmentation for
  methylation count data}

\author{Toby Dylan Hocking}

\maketitle

\section{Introduction/problem}

Warren says that methylation sequencing data along the genome can be
represented as a pair $(t_j, m_j)$ where $t_j\in\ZZ_+$ is the total
number of reads at methylation site $j$ and $m_j\in\{0,1,\dots, t_j\}$
is the number of methylated reads. The goal in analyzing these data is
to classify the genome into hyper-methylated regions with high $m_j$
values, and hypo-methylated regions with low $m_j$ values.

\section{Binomial segmentation model}

We propose the model $m_j \sim \text{Binomial}(t_j, p_j)$. The
binomial distribution has the probability mass function
\begin{equation}
  \label{eq:binomial-prob}
  \text{Prob}(m, p) = \binom{t}{m} p^m (1-p)^{t-m}
\end{equation}
which means the loss function for site $j$ is
\begin{equation}
  \label{eq:binomial-loss}
  \ell_j(p) = (m_j - t_j) \log(1-p) - m_j\log p.
\end{equation}

\section{Standard dynamic programming algorithm}

Let $(j', j]$ denote the interval of indices including $j'+1$ up to
and including $j$. Let $t_{(j', j]} = \sum_{i=j'+1}^{j} t_i$ be the
cumulative sum of total reads on that interval; idem for $m_{(j',
  j]}$. The optimal probability of methylation on that segment is
$p_{(j', j]} = m_{(j', j]}/t_{(j', j]}$, and the optimal loss of that
segment is
\begin{equation}
  \label{eq:optimal-cost}
  c_{(j', j]} = 
  ( m_{(j', j]} - t_{(j', j]} )
  \log\left\{ 
    1 - p_{(j', j]}
  \right\}
  - m_{(j', j]}
  \log p_{(j', j]}.
\end{equation}

Let $\mathcal L_{s,j}$ be the optimal loss in $s$ segments up to data
point $j$. To compute $\mathcal L_{1, j}$ we first compute and store
the $d$-vectors of cumulative sums $m_{(0, j]}$ and $t_{(0, j]}$, for
all sites $j\in\{1, \dots, d\}$.

The DPA computes the optimal loss for all segments $s\in\{2, \dots,
s_{\text{max}} \}$ and for all sites $j\in\{1, \dots, d\}$ in
$O(s_{\text{max}} d^2)$ time. This is accomplished using the
recursion, for $s>1$,
\begin{equation}
  \label{eq:dpa-recursion}
  \mathcal L_{s, j} = 
  \min_{j' < j}
  \mathcal L_{s-1, j'}
  + 
  c_{(j', j]}.
\end{equation}
Note that the optimal cost $c_{(j', j]}$ may be computed in constant
$O(1)$ time given the $d$-vectors of cumulative sums $m_{(0, j]}$ and
$t_{(0, j]}$.

\section{Pruned dynamic programming algorithm}

To use the pDPA of \citet{pruned-dp} for the binomial distribution we
need to be able to find the solution of
\begin{equation}
  \label{eq:pDPA-intervals}
  a \log(1-p) + b \log p + c = d
\end{equation}
in terms of $p$, where $a,b$ are positive integers. Mathematica does
not provide a closed form solution. It can be re-written as
\begin{equation}
  \label{eq:pDPA-interval-factorized}
  e^{d-c} = (1-p)^a p^b,
\end{equation}
which can be further simplified (using the binomial theorem, since $a$
is a positive integer) to
\begin{equation}
  \label{eq:pDPA-interval-polynomial}
  e^{d-c} = \sum_{i=0}^a \binom{a}{i} (-1)^{a-i} p^{a-i+b},
\end{equation}
which makes it clear that the right side is a polynomial of degree
$a+b$. By the Abel-Ruffini theorem \citep{Abel-Ruffini-wikipedia}, if
the polynomial degree $a+b\geq 5$, then it is possible that there is
no closed-form solution in terms of radicals. However we can always
use an iterative root-finder such as Newton's method.

\includegraphics[width=\textwidth]{figure-binomial-loss}

The figure above shows the binomial loss function $\ell_j(p)$ for
$t_j\in\{10, 100\}$ (rows) and $m_j=\hat p t_j$ with $\hat p\in\{0.3,
0.5\}$ (columns). It is clear that the loss is convex, so there is
either 0, 1, or 2 intersection points with a constant function
(colored horizontal line). Finding the intersection points is a key
subroutine in the pDPA, and it seems that we will need to find the two
intersection points using a root-finder.

\section{Root finder}

From experience implementing a root finder for the Poisson loss, it
will be advantageous (faster convergence) to do the root finding in
another space, where the tails are linear.

Let $u=\log p - \log(1-p) = \log(\frac{p}{1-p})\in\RR$ be that new
variable, so $p=(1+e^{-u})^{-1}$. Thus
\begin{eqnarray*}
  \ell_j(p) 
  &=& (m_j - t_j) \log(1-p) - m_j\log p \\
  &=& (m_j - t_j) \log(1-\frac{1}{1+e^{-u}}) - m_j\log(\frac{1}{1+e^{-u}})\\
g_j(u)  &=& (t_j - m_j) \log(e^u+1) + m_j\log(1+e^{-u}) \\
\end{eqnarray*}
The equation above can be used to update/add the cost of a new data
point. We will thus need to store and find roots of functions such as
the one below, 
\begin{equation}
  \label{eq:g(u)}
  g(u)=a\log(1+e^u)+b\log(1+e^{-u})+c=d,
\end{equation}
where we will store the coefficients $a,b,c$ as double
precision floating point numbers. The derivatives are
\begin{equation}
  \label{eq:g'(u)}
  g'(u)=\frac{ae^u - b}{1+e^u} = \frac{a-be^{-u}}{e^{-u}+1}.
\end{equation}
The equations above make it clear that the asymptotes are linear. As
$u\rightarrow\infty$, $g'(u)\rightarrow a$. As $u\rightarrow -\infty$,
$g'(u)\rightarrow -b$. They also can be used to show that the minimum
of the loss function occurs at $u=\log(b/a)$.  Depending on whether we
are looking for the smaller or larger root, we can start the root
finding on one side or the other of the minimum:
$u_0 = \log(b/a)\pm 1$. Then the updates are for each iteration $i>0$,
$u_i = u_{i-1} - g(u_{i-1})/g'(u_{i-1})$. We can stop the root when
the cost is below some small absolute value, such as $10^{-12}$. For
large data we should store the mean cost rather than the total cost,
for numerical stability.

The second derivative below is probably not used in
the PDPA but since it is positive it is clear that the loss function
is convex:
\begin{equation}
  \label{eq:g''(u)}
  g''(u)=\frac{(a+b)e^u}{(1+e^u)^2} > 0.
\end{equation}


\section{Warren's public data set 26 Feb 2015}

\includegraphics[width=\textwidth]{figure-public-small-data}

The figure above shows some manually annotated regions that Warren
created on a public data set. Do we want to label every base as
hypo/hyper, or should we leave some bases as normal/unlabeled?

\includegraphics[width=\textwidth]{figure-public-small}

The figure above shows the same data set but with 3 optimal binomial
segmentation models of varying complexity. How to choose the optimal
number of segments?

\section{Data on abacus}

\begin{verbatim}
/lb/project/mugqic/epigenome/pipelines/wgb_seq/v_1/
Methylation data is in 
PROJECT/hg19/methylation/*.profile.cg_strand_combined.csv

K27M is one class and SETD2IDH1 is another class?

Yes - here are the current designations for EMC_ICHANGE.  
WT means tumour that does not have any of the other known mutations.

The ones in the EGAD000.... directory were downloaded from 
(access controlled) online repo for another paper/collaborator.  
The WT here are not 100% confirmed - 
SETD2 hasn't been checked for these and the other calls 
were made from WGBS and not verified elsewhere
(except for K27M which was given by
the original authors in the paper).
\end{verbatim}



\bibliographystyle{abbrvnat}

\bibliography{refs}

\end{document}
